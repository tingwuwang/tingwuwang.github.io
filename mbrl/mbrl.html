<!DOCTYPE HTML PUBLIC "-//W3C//DTD HTML 4.01 Transitional//EN">
<html>

	<script src="http://www.google.com/jsapi" type="text/javascript"></script>
	<script type="text/javascript">google.load("jquery", "1.3.2");</script>

	<style type="text/css">
body {
	font-family: "Titillium Web","HelveticaNeue-Light", "Helvetica Neue Light", "Helvetica Neue", Helvetica, Arial, "Lucida Grande", sans-serif;
	font-weight:300;
	font-size:18px;
	margin-left: auto;
	margin-right: auto;
	width: 980px;
}

  h1 {
	  font-weight:300;
  }

  .disclaimerbox {
	  background-color: #eee;
	  border: 1px solid #eeeeee;
	  border-radius: 10px ;
	  -moz-border-radius: 10px ;
	  -webkit-border-radius: 10px ;
	  padding: 20px;
  }

  video.header-vid {
	  height: 140px;
	  border: 1px solid black;
	  border-radius: 10px ;
	  -moz-border-radius: 10px ;
	  -webkit-border-radius: 10px ;
  }

  img.header-img {
	  height: 140px;
	  border: 1px solid black;
	  border-radius: 10px ;
	  -moz-border-radius: 10px ;
	  -webkit-border-radius: 10px ;
  }

  img.rounded {
	  border: 1px solid #eeeeee;
	  border-radius: 10px ;
	  -moz-border-radius: 10px ;
	  -webkit-border-radius: 10px ;
  }

  a:link,a:visited
	  {
		  color: #1367a7;
		  text-decoration: none;
	  }
	  a:hover {
		  color: #208799;
	  }

	  td.dl-link {
		  height: 160px;
		  text-align: center;
		  font-size: 22px;
	  }

	  .layered-paper-big { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		  box-shadow:
			  0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
			  5px 5px 0 0px #fff, /* The second layer */
			  5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
			  10px 10px 0 0px #fff, /* The third layer */
			  10px 10px 1px 1px rgba(0,0,0,0.35), /* The third layer shadow */
			  15px 15px 0 0px #fff, /* The fourth layer */
			  15px 15px 1px 1px rgba(0,0,0,0.35), /* The fourth layer shadow */
			  20px 20px 0 0px #fff, /* The fifth layer */
			  20px 20px 1px 1px rgba(0,0,0,0.35), /* The fifth layer shadow */
			  25px 25px 0 0px #fff, /* The fifth layer */
			  25px 25px 1px 1px rgba(0,0,0,0.35); /* The fifth layer shadow */
		  margin-left: 10px;
		  margin-right: 45px;
	  }


	  .layered-paper { /* modified from: http://css-tricks.com/snippets/css/layered-paper/ */
		  box-shadow:
			  0px 0px 1px 1px rgba(0,0,0,0.35), /* The top layer shadow */
			  5px 5px 0 0px #fff, /* The second layer */
			  5px 5px 1px 1px rgba(0,0,0,0.35), /* The second layer shadow */
			  10px 10px 0 0px #fff, /* The third layer */
			  10px 10px 1px 1px rgba(0,0,0,0.35); /* The third layer shadow */
		  margin-top: 5px;
		  margin-left: 10px;
		  margin-right: 30px;
		  margin-bottom: 5px;
	  }

	  .vert-cent {
		  position: relative;
		  top: 50%;
		  transform: translateY(-50%);
	  }

	  hr
	  {
		  border: 0;
		  height: 1px;
		  background-image: linear-gradient(to right, rgba(0, 0, 0, 0), rgba(0, 0, 0, 0.75), rgba(0, 0, 0, 0));
	  }
	</style>
	<!-- Start : Google Analytics Code -->
	<script>
		(function(i,s,o,g,r,a,m){i['GoogleAnalyticsObject']=r;i[r]=i[r]||function(){
			(i[r].q=i[r].q||[]).push(arguments)},i[r].l=1*new Date();a=s.createElement(o),
				m=s.getElementsByTagName(o)[0];a.async=1;a.src=g;m.parentNode.insertBefore(a,m)
		})(window,document,'script','//www.google-analytics.com/analytics.js','ga');
ga('create', 'UA-64069893-1', 'auto');
ga('send', 'pageview');
	</script>
	<!-- End : Google Analytics Code -->
	<script type="text/javascript" src="../js/hidebib.js"></script>
	<link href='https://fonts.googleapis.com/css?family=Titillium+Web:400,600,400italic,600italic,300,300italic' rel='stylesheet' type='text/css'>
	<head>
		<title>Benchmarking Model-Based Reinforcement Learning</title>
		<meta property="og:description" content="Benchmarking Model-Based Reinforcement Learning"/>
	</head>

	<body>
		<br>
		<center><span style="font-size:46px;font-weight:bold;">Benchmarking Model-Based Reinforcement Learning</span></center>
		<br>

		<table align=center width=800px>
            <tr>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="http://www.cs.toronto.edu/~tingwuwang">Tingwu Wang</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://scholar.google.com/citations?user=gnBwBZ4AAAAJ&hl=en">Xuchan Bao</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://iclavera.github.io/">Ignasi Clavera</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://www.linkedin.com/in/jerrick-hoang-06093035/">Jerrick Hoang</a></span></center></td>
            </tr>

        </table>
        <table align=center width=800px>
            <tr>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="http://www.cs.toronto.edu/~ywen/">Yeming Wen</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://www.linkedin.com/in/edtsft/?originalSubdomain=ca">Eric Langlois</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://www.linkedin.com/in/matthew-zhang-218253140/?originalSubdomain=ca">Shunshi Zhang</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="http://www.cs.toronto.edu/~gdzhang/">Guodong Zhang</a></span></center></td>
            </tr>

        </table>
        <table align=center width=500px>
            <tr align=center>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://people.eecs.berkeley.edu/~pabbeel/">Pieter Abbeel</a></span></center></td>
                <td align=center width=100px>
                    <center><span style="font-size:24px"><a href="https://jimmylba.github.io/">Jimmy Ba</a></span></center></td>
            </tr>
        </table>
        <table align=center width=600px>
            <tr>
                <td align=center width=500px><center><span style="font-size:24px">
                            University of Toronto&nbsp;&nbsp;  &amp; &nbsp;&nbsp;UC Berkeley&nbsp;&nbsp;&amp; &nbsp;&nbsp;Vector Institute</span></center>
                </td>
            </tr>
		</table>
		</center>

		<table align=center width=1200px>
			<center><a href="mbbl_front.png"><img class="rounded" src="mbbl_front.png" height="380px"></img></a><br></center>
		</table>

		<br>
        <a href='https://arxiv.org/abs/1907.02057'>[Arxiv Page]</a> <a href='https://arxiv.org/pdf/1907.02057.pdf'>[PDF]</a> Model-based reinforcement learning (MBRL) is widely seen as having the potential to be significantly more sample efficient than model-free RL. 
		However, research in model-based RL has not been very standardized.
        It is fairly common for authors to experiment with self-designed environments, and there are several separate lines of research,
        which are sometimes closed-sourced or not reproducible.
        Accordingly, it is an open question how these various existing MBRL algorithms perform relative to each other.
		To facilitate research in MBRL,
		in this paper we gather a wide collection of MBRL algorithms and propose over 18 benchmarking environments specially designed for MBRL.
		We benchmark these MBRL algorithms with unified problem settings, including noisy environments.
        Beyond cataloguing performance, we explore and unify the underlying algorithmic differences across MBRL algorithms.
		We characterize three key research challenges for future MBRL research:
		the dynamics coupling effect,
		the planning horizon dilemma, and the early-termination dilemma.
		<br>
		<br>
		<hr>
		<center id="sourceCode"><h1>Algorithms</h1></center>
		<font size="5"><b>Dyna-Style Algorithms</b></font><br>
		In the Dyna algorithm, training iterates between two steps. First, using the current
		policy, data is gathered from interaction with the environment and then used to learn the dynamics
		model. Second, the policy is improved with imagined data generated by the learned model. This class
		of algorithms learn policies using model-free algorithms with rich imaginary experience without
		interaction with the real environment.<br>

		(1) <a href='https://arxiv.org/abs/1802.10592'>Model-Ensemble Trust-Region Policy Optimization (ME-TRPO)</a><br>
		(2) <a href='https://arxiv.org/abs/1807.03858'>Stochastic Lower Bound Optimization (SLBO)</a><br>
		(3) <a href='https://arxiv.org/abs/1809.05214'>Model-Based Meta-Policy-Optimzation (MB-MPO)</a><br>

		<font size="5"><b>Policy Search with Backpropagation through Time</b></font><br>
		Contrary to Dyna-style algorithms, where the learned dynamics models are used to provide imagined
		data, policy search with backpropagation through time exploits the model derivatives. Consequently,
		these algorithms are able to compute the analytic gradient of the RL objective with respect to the
		policy, and improve the policy accordingly.<br>

		(4) <a href='https://www.ias.informatik.tu-darmstadt.de/uploads/Publications/Deisenroth_ICML_2011.pdf'>Probabilistic Inference for Learning Control (PILCO)</a><br>
		(5) <a href='https://homes.cs.washington.edu/~todorov/papers/TassaIROS12.pdf'>Iterative Linear Quadratic-Gaussian (iLQG)</a><br>
		(6) <a href='http://rll.berkeley.edu/gps/'>Guided Policy Search (GPS)</a><br>
		(7) <a href='https://arxiv.org/abs/1510.09142'>Stochastic Value Gradients (SVG)</a><br>

		<font size="5"><b>Shooting Algorithms </b></font><br>
		This class of algorithms provide a way to approximately solve the receding horizon problem posed
		in model predictive control (MPC) when dealing with non-linear dynamics and non-convex reward
		functions. Their popularity has increased with the use of neural networks for modelling dynamics. <br>
		(8) <a href='http://www.anilvrao.com/Publications/ConferencePublications/trajectorySurveyAAS.pdf'>Random Shooting (RS)</a><br>
		(9) <a href='https://arxiv.org/abs/1708.02596'>Mode-Free Model-Based (MB-MF)</a><br>
		(10) <a href='https://arxiv.org/abs/1805.12114'>Probabilistic Ensembles with Trajectory Sampling (PETS-RS and PETS-CEM)</a><br>

		<font size="5"><b>Model-free Baselines</b></font><br>
		In our benchmark, we include MFRL baselines to quantify the sample complexity and asymptotic
		performance gap between MFRL and MBRL. 
		We include both state-of-the-art on-policy and off-policy algorithms. <br>
		(11) <a href='https://arxiv.org/abs/1502.05477'>Trust-Region Policy Optimization (TRPO)</a><br>
		(12) <a href='https://arxiv.org/abs/1707.06347'>Proximal-Policy Optimization (PPO)</a><br>
		(13) <a href='https://arxiv.org/pdf/1802.09477'>Twin Delayed Deep Deterministic Policy Gradient (TD3)</a><br>
		(14) <a href='https://arxiv.org/abs/1801.01290'>Soft Actor-Critic (SAC)</a><br>

		</hr>
		<hr>
		<center id="sourceCode"><h1>Results</h1></center>
		Our benchmark consists of 18 environments with continuous state and action space based on OpenAI
		Gym. We include a full spectrum of environments with different difficulty and episode length,
		from CartPole to Humanoid.
		Each algorithm is run with 4 random seeds for 200k time-steps.
		We refer readers for the paper for detailed performance curves.
        <table align=center width=1100px>
            <center><a href="mbbl_result_table.jpeg"><img src="mbbl_result_table.jpeg" height="1100px"></img></a><br></center>
        </table>
		</hr>
		
		</hr>
		<hr>
		<center id="sourceCode"><h1>Engineering Statistics</h1></center>
        The engineering statistics shown in Table 2 include the computational resources,
        the estimated wall-clock time, and whether the algorithm is fast enough
        to run at real-time at test time, namely, if the action selection can be done faster than the default
        time-step of the environment.
        <table align=center width=1100px>
            <center><a href="mbbl_stats.png"><img src="mbbl_stats.png" height="320px"></img></a><br></center>
        </table>
		</hr>

		<hr>
		<center id="sourceCode"><h1>Dynamics Bottleneck</h1></center>
		The results show that MBRL algorithms plateau at a performance level well below their model-free
		counterparts and themselves with ground-truth dynamics. This points out that when learning models,
		more data does not result in better performance. For instance, PETS's performance plateaus after
		400k time-steps at a value much lower than the performance when using the ground-truth dynamics.
        <table align=center width=1100px>
            <center><a href="1m_mbbl_result_table.jpeg"><img src="1m_mbbl_result_table.jpeg" height="400px"></img></a><br></center>
        </table>
		</hr>
		
		<hr>
		<center id="sourceCode"><h1>Planning Horizon Dilemma</h1></center>
		One of the critical choices in shooting methods is the
		planning horizon. In Figure 3, we show the performance
		of iLQG, CEM and RS, using the same number of candidate planning sequences, but with different planning
		horizon. We notice that increasing the planning horizon
		does not necessarily increase the performance, and more often instead decreases the performance. 
		We argue that this is result of insufficient planning in a search space which increases exponentially with
		planning depth, i. e., the curse of dimensionality. <br>
		On the right, we further experiment with the imaginary environment length in SLBO (Dyna) algorithms.
		We have similar results that increasing horizon does not necessarily help the performance and sometimes hurt the performance.
        <table align=center width=1100px>
            <center><a href="depth_dilemma.jpeg"><img src="depth_dilemma.jpeg" height="290px"></img></a><br></center>
        </table>
		</hr>
		
		<hr>
		<center id="sourceCode"><h1>Early Termination Dilemma</h1></center>
		Early termination, when the episode is finalized before the horizon has been reached, is a standard
		technique used in MFRL algorithms to prevent the agent from visiting unpromising states or damaging
		states for real robots. When early termination is applied to the real environments, MBRL can
		correspondingly also apply early termination in the planned trajectories, or generate early terminated
		imaginary data. However, we find this technique hard to integrate into the existing MB algorithms.
		</hr>

		<hr>
		<center id="sourceCode"><h1>Conclusion</h1></center>
		In this paper, we benchmark the performance of a wide collection of existing MBRL algorithms, evaluating their sample efficiency,
        asymptotic performance and robustness. Through systematic evaluation
		and comparison, we characterize three key research challenges for future MBRL research. Across
		this very substantial benchmarking, there is no clear consistent best MBRL algorithm, suggesting lots
		of opportunities for future work bringing together the strengths of different approaches.
		</hr>

		<hr>
		<center id="sourceCode"><h1>Reproducibility and MBRL Package</h1></center>
		The benchmarking environments and the code to reproduce the results can be found <a href='https://github.com/WilsonWangTHU/mbbl'>Github Code</a>.
        The installation guide and detailed readme to run the code can be found in the Github.
		We are currently working towards a unified package for all the MBRL algorithms.
		</hr>
		
		<hr>
        <center id="sourceCode"><h1><a href='mbrl.pdf'>[Paper] (5.7 MB)</a></h1></center>
        <table align=center width=1100px>
            <center><a href="mbbl_thumbnail.png"><img src="mbbl_thumbnail.png" height="800px"></img></a><br></center>
        </table>
		</hr>

        <table width="100%" cellspacing="0" cellpadding="20" border="0" align="center">
            <tbody><tr><td><br><p align="right"><font size="2">
                            Last Update: Jun 26, 2019<br>
                            Web Page Template: <a href="https://pathak22.github.io/noreward-rl/">this</a>
                        </font></p></td></tr>
            </tbody></table>

	</body>
</html>

